{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0218done_Homework_3_Spr2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# ECE795 Advanced Big Data Analytics Homework 3 (Due 2/24)\n",
        "\n",
        "Set up Pyspark Environment.\n",
        "\n",
        "Tips for Colab:\n",
        "\n",
        "1. You will be disconnected if you are idle for more than 90 minutes and will be mandatorily disconnected after 12 hour connection. \n",
        "\n",
        "2. Once you got disconnected, you need to execute the codes from the beginning to setup the environment again.\n",
        "\n",
        "3. For the purpose of homework, it should be sufficient since each problem should not take more than 5 minutes to generate the results.\n",
        "\n",
        "4. To facilitate the use of Colab, you can use \"MainMenu - Runtime - Run all” to run all the cells in the notebook. So you do not have to click each cell to setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278e5199-6a65-41c4-97b2-5fdd9c4ee930"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 64.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=67e4d789607441476f041577786744a58954f3edbd6dd13dcc129415546b420c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark.\n",
        "\n",
        "# Read input text file to RDD \n",
        "\n",
        "Download the input data from using the following command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37fda59-b800-4edf-89e5-eab505aa21ab"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/umddm/ECE795_Homeworks_Spring2022/homework_3/Gutenberg.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-23 01:56:38--  https://raw.githubusercontent.com/umddm/ECE795_Homeworks_Spring2022/homework_3/Gutenberg.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3536418 (3.4M) [text/plain]\n",
            "Saving to: ‘Gutenberg.txt’\n",
            "\n",
            "Gutenberg.txt       100%[===================>]   3.37M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-02-23 01:56:38 (73.8 MB/s) - ‘Gutenberg.txt’ saved [3536418/3536418]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Now that we have input data, we can start to do the homework. \n",
        "\n",
        "## Question 1 (10 points): The following RDD, 'words', is obtained from Homework 2, Question 3. It contains all the words in the text file, 'Gutenberg.txt', after pre-processing. Using this RDD, please create another RDD called 'frequencies' that contains the term frequencies of all words.\n",
        "\n",
        "### Example output from 'frequencies.take(10)':\n",
        "```\n",
        "[('project', 95),\n",
        " ('ten', 208),\n",
        " ('use', 35),\n",
        " ('anyone', 5),\n",
        " ('anywhere', 6),\n",
        " ('united', 20),\n",
        " ('world', 39),\n",
        " ('whatsoever', 2),\n",
        " ('may', 84),\n",
        " ('give', 155)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf381ba8-6f47-4878-abd7-fdd91e025709"
      },
      "source": [
        "#Question_1:\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "def removeNonAlpabet(s):\n",
        "    return ''.join([i.lower() for i in s if i.isalpha() or i==' ']).lstrip().rstrip()\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sentences = sc.textFile('Gutenberg.txt').map(removeNonAlpabet).filter(lambda x: x!='')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1\n",
        "#Fill out here\n",
        "\n",
        "words = sentences.flatMap(lambda line: line.split())\n",
        "frequencies = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)  # .sortBy(lambda x: -x[1])\n",
        "# test\n",
        "frequencies.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dBJsz0EUg-8",
        "outputId": "bf433618-582a-4961-8665-8780477aba98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('project', 95),\n",
              " ('gutenberg', 32),\n",
              " ('ebook', 13),\n",
              " ('of', 6820),\n",
              " ('engelsch', 24),\n",
              " ('ten', 208),\n",
              " ('bruggencate', 2),\n",
              " ('this', 494),\n",
              " ('is', 2621),\n",
              " ('use', 35)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "## Question 2 (10 points): For the 'frequencies' RDD in Question 1, normalize the values of term frequencies so that for every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets converted into a decimal between 0 and 1 (also referred to as Min-Max normalization).\n",
        "\n",
        "The min-max normalized term frequency of the $n^{th}$ word can be calculated as:\n",
        "\n",
        "## $\\left\\Vert f_n \\right\\Vert_{minmax} = \\frac{f_n-\\min{f}}{\\max{f}-\\min{f}}$\n",
        "\n",
        "where $f_n$ and $\\left\\Vert f_n \\right\\Vert_{minmax}$ are the term frequencies of the $n^{th}$ word before and after min-max normalization; $\\min{f}$,$\\max{f}$ are the minimum and maximum frequencies before normalization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question_2\n",
        "#Fill out here\n",
        "\n",
        "import numpy as np\n",
        "words = sentences.flatMap(lambda line: line.split())\n",
        "frequencies = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)  # .sortBy(lambda x: -x[1])  [('the', 1.0),('subst', 0.9920370638482698)]\n",
        "frequencies_list = np.array(frequencies.map(lambda x: x[1]).collect())\n",
        "f_max, f_min = frequencies_list.max(), frequencies_list.min()\n",
        "f_maxmin = frequencies.map(lambda x: (x[0], ((x[1] - f_min)) / (f_max - f_min)))\n",
        "# f_maxmin.collect()\n",
        "# test\n",
        "f_maxmin.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwnTwFuSCotE",
        "outputId": "90292523-d064-4822-f7bc-1bcdb4e5d50c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('project', 0.013609381786593312),\n",
              " ('gutenberg', 0.004488200376429709),\n",
              " ('ebook', 0.00173736788765021),\n",
              " ('of', 0.9872593021572318),\n",
              " ('engelsch', 0.0033299551179962355),\n",
              " ('ten', 0.02996959606196612),\n",
              " ('bruggencate', 0.00014478065730418415),\n",
              " ('this', 0.07137686405096279),\n",
              " ('is', 0.3793253221369625),\n",
              " ('use', 0.0049225423483422615)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHVHVQjd4P1"
      },
      "source": [
        "## Question 3 (10 points): For the 'frequencies' RDD in Question 1, normalize the values of term frequencies through Z-Score Normalization.\n",
        "\n",
        "The Z-Score Normalization term frequency of the $n^{th}$ word can be calculated as:\n",
        "\n",
        "## $\\left\\Vert f_n \\right\\Vert_z = \\frac{f_n - \\mu}{\\sigma}$\n",
        "\n",
        "where $\\mu$ is the mean value of the feature and $\\sigma$ is the standard deviation of the feature. If a value is exactly equal to the mean of all the values of the feature, it will be normalized to 0. If it is below the mean, it will be a negative number, and if it is above the mean it will be a positive number. The size of those negative and positive numbers is determined by the standard deviation of the original feature. If the unnormalized data had a large standard deviation, the normalized values will be closer to 0."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question_3\n",
        "#Fill out here\n",
        "\n",
        "import numpy as np\n",
        "words = sentences.flatMap(lambda line: line.split())\n",
        "frequencies = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y) # .sortBy(lambda x: -x[1])\n",
        "frequencies_list = np.array(frequencies.map(lambda x: x[1]).collect())\n",
        "f_mu, f_sigma = frequencies_list.mean(), frequencies_list.std()\n",
        "f_zscore = frequencies.map(lambda x: (x[0], (x[1] - f_mu) / f_sigma))\n",
        "# f_zscore.collect()\n",
        "# test\n",
        "f_zscore.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvOksNiy_dEX",
        "outputId": "85e7fd31-b3bb-4357-852a-8979d64641b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('project', 1.3788788810367412),\n",
              " ('gutenberg', 0.4212583524560268),\n",
              " ('ebook', 0.13245216129676377),\n",
              " ('of', 103.60107022556537),\n",
              " ('engelsch', 0.2996557456521266),\n",
              " ('ten', 3.096515702141832),\n",
              " ('bruggencate', -0.034751423058599046),\n",
              " ('this', 7.443808895381264),\n",
              " ('is', 39.77490197936824),\n",
              " ('use', 0.46685933000748936)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}